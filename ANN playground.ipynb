{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\n# fix random seed for reproducibility\nseed = 7\nnp.random.seed(seed)\n\nCommitFlag = True\nenable_pca = False\n\nif not CommitFlag:\n    print(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# extract the training data\ndataset = pd.read_csv('../input/train.csv')\nX = dataset.iloc[:, 1:].values\ny = dataset.iloc[:, :1].values\n\ntest_dataset = pd.read_csv('../input/test.csv')\ntest = test_dataset.iloc[:,:].values\n\n# There are 42000 images so 42000 labels\nprint(\"The shape of y: {}\".format(y.shape))\nm = y.shape[0]\nprint(\"The number of images: m = {}\".format(m))\ninput_count = X.shape[1]\nprint(\"The size of input: m = {}\".format(input_count))\n\n\n# feature scaling\n#X = X/255.0\n#test = test/255.0\n\nX = X.reshape(-1,28,28,1)\ntest = test.reshape(-1,28,28,1)\n\n# PCA\nfrom sklearn.decomposition import PCA\n\nif enable_pca:\n    pca_component_count = 300\n    pca = PCA(n_components=pca_component_count, whiten=True)\n    pca.fit(X)\n    X = pca.transform(X)\n    test = pca.transform(test)\n    \nprint(X.shape)\nprint(test.shape)\n\n#print(y[:10])\nif not CommitFlag:\n    sns.countplot(np.squeeze(y))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check IF some Feature variables are NaN\nif not CommitFlag:\n    np.unique(np.isnan(X))[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check IF some Target Variables are NaN\nif not CommitFlag:\n    np.unique(np.isnan(y))[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical data\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder(categorical_features = [0])\ny = onehotencoder.fit_transform(y).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.constraints import maxnorm\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import SGD\n\n# With data augmentation to prevent overfitting (accuracy 0.99286)\nfrom keras.preprocessing.image import ImageDataGenerator\n# Set a learning rate annealer\nfrom keras.callbacks import ReduceLROnPlateau\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nets_count = 15\n\n# Fitting the ANN to the Training set\nepochs_count = 0\nbatch_size_vl = 64\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\nmodel = [0] * nets_count\nhistory = [0] * nets_count\n\nfor j in range(0,nets_count):    \n    if CommitFlag:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.0)\n    else:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,stratify=y)\n    print(X_train.shape)\n\n    # Initialising the ANN\n    model[j] = Sequential()\n    weight_max = 3\n\n    output_count = y.shape[1]\n    print(\"input_count: \"+str(input_count))\n    print(\"output_count: \" + str(output_count))\n    #Add the first hidden layer, and specifying #inputs mean(10,784)=397)\n    hidden_layers_count = 2\n    #delta = (input_count-output_count)/ (hidden_layers_count+1)\n    #nh = int(input_count - delta)\n    nh = int(((input_count+output_count)/2))\n    dropout_prob = 0.3\n    #pdelta = (dropout_prob - 0.01)/ (hidden_layers_count)\n    #pdelta = 0.2\n    \n    model[j] = Sequential()\n\n    model[j].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(32, kernel_size = 3, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n\n    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n\n    model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Flatten())\n    model[j].add(Dropout(0.4))\n    model[j].add(Dense(10, activation='softmax'))\n\n    # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    \n    model[j].summary()\n    \n    # Compiling the ANN\n    #model.compile(optimizer = SGD(lr=0.015, momentum=0.8, decay=0.0, nesterov=True), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    #model[j].compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    #X_train = X_train.reshape(-1,28,28,1)\n    datagen = ImageDataGenerator(\n#            featurewise_center=True,  # set input mean to 0 over the dataset\n#            samplewise_center=False,  # set each sample mean to 0\n#            featurewise_std_normalization=True,  # divide inputs by std of the dataset\n#            samplewise_std_normalization=False,  # divide each input by its std\n#            zca_whitening=True,  # apply ZCA whitening\n            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n            zoom_range = 0.1, # Randomly zoom image \n            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n            horizontal_flip=False,  # randomly flip images\n            vertical_flip=False)  # randomly flip images\n\n\n    datagen.fit(X_train)\n    \n    if CommitFlag:\n        epochs_count = 45\n        history[j] = model[j].fit_generator(datagen.flow(X_train,y_train, batch_size = batch_size_vl),\n                             epochs = epochs_count, \n                             steps_per_epoch=m// batch_size_vl, \n                             callbacks=[learning_rate_reduction])\n    else:\n        epochs_count = 25\n        history[j] = model[j].fit_generator(datagen.flow(X_train,y_train, batch_size = batch_size_vl),\n                             validation_data = (X_test, y_test), \n                             epochs = epochs_count, \n                             steps_per_epoch=m//batch_size_vl, \n                             callbacks=[learning_rate_reduction])\n\n\n    \n    if not CommitFlag:\n        print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(j+1,\n                                                                                            epochs_count,max(history[j].history['acc']),\n                                                                                            max(history[j].history['val_acc']) ))\n        model_acc = model[j].evaluate(X_test, y_test)\n        print(\" Model Accuracy is : {0:.1f}%\".format(model_acc[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"    model[j].add(MaxPool2D(pool_size=(1,1), input_shape = (28,28,1)))\n    model[j].add(Flatten())\n    for i in range(1,hidden_layers_count+1):\n        print(\"nh: \" + str(nh))\n        print(\"dropout_prob: \" + str(dropout_prob))\n        model[j].add(Dense(units=nh,kernel_initializer='uniform',activation='relu',input_dim=input_count,kernel_constraint=maxnorm(weight_max)))\n        model[j].add(Dropout(dropout_prob))\n        nh = int(nh/2)\n        #nh = int(nh - delta)\n        #dropout_prob = dropout_prob / 2.0\n        #dropout_prob = dropout_prob - pdelta\n    #nh2 = int(nh1/2)\n    #print(\"nh2: \" + str(nh2))\n\n    #Add the output layer, an analog digit value\n    model[j].add(Dense(units=10,kernel_initializer='uniform',activation='sigmoid'))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ENSEMBLE PREDICTIONS AND SUBMIT\nresults = np.zeros( (test.shape[0],10) ) \nfor j in range(nets_count):\n    results = results + model[j].predict(test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submission.csv\",index=False)\nsubmission.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"for i in range(1,10):\n    index = np.random.randint(1,28001)\n    plt.subplot(3,3,i)\n    plt.imshow(test[index].reshape(28,28))\n    plt.title(\"Predicted Label : {}\".format(results[index]))\nplt.subplots_adjust(hspace = 1.2, wspace = 1.2)\nplt.show()\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Look at confusion matrix \nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n   # This function prints and plots the confusion matrix.\n   # Normalization can be applied by setting `normalize=True`.\n   \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nif not CommitFlag:\n    # Predict the values from the validation dataset\n    Y_pred = model.predict(X_test)\n    # Convert predictions classes to one hot vectors \n    Y_pred_classes = np.argmax(Y_pred,axis = 1) \n    # Convert validation observations to one hot vectors\n    Y_true = np.argmax(y_test,axis = 1) \n    # compute the confusion matrix\n    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n    # plot the confusion matrix\n    plot_confusion_matrix(confusion_mtx, classes = range(10)) \n    \n# Display some error results \n\n# Errors are difference between predicted labels and true labels\nif not CommitFlag:\n\n    errors = (Y_pred_classes - Y_true != 0)\n\n    Y_pred_classes_errors = Y_pred_classes[errors]\n    Y_pred_errors = Y_pred[errors]\n    Y_true_errors = Y_true[errors]\n    X_val_errors = X_test[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    # This function shows 6 images with their predicted and real labels\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\nif not CommitFlag:\n    # Probabilities of the wrong predicted numbers\n    Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n    # Predicted probabilities of the true values in the error set\n    true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n    # Difference between the probability of the predicted label and the true label\n    delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n    # Sorted list of the delta prob errors\n    sorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n    # Top 6 errors \n    most_important_errors = sorted_dela_errors[-6:]\n\n    # Show the top 6 errors\n    display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)\n    \"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}